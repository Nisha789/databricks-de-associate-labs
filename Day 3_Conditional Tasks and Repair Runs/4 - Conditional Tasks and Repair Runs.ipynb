{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c154d81f-cecc-4a43-be16-3ce1509fe791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d51cbbc2-1e41-4b39-a1c7-7ceea4b446db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Conditional Tasks and Repair Runs\n",
    "\n",
    "Databricks Workflow Jobs have the ability to run tasks based on the result of previously run tasks. For example, you can setup a task to only run if a previous task fails.\n",
    "\n",
    "Also, when a task fails, you can repair the run and restart tasks without restarting the whole job. This can save a significant amount of time.\n",
    "\n",
    "In this lesson, we will configure a pipeline with conditional logic, and we will learn how to repair runs. \n",
    "\n",
    "**Our goal is to create this job:**\n",
    "\n",
    "![Lesson04_FullJobRun](files/images/deploy-workloads-with-databricks-workflows-2.0.2/Lesson04_FullJobRun.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e70f79b-4b3e-4f93-b25e-cda2255f74b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "  - In the drop-down, select **More**.\n",
    "\n",
    "  - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73b3354a-d14b-4369-b608-eba67912963f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to **dbacademy** and the schema to your specific schema name shown below using the `USE` statements.\n",
    "<br></br>\n",
    "```\n",
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA dbacademy.<your unique schema name>;\n",
    "```\n",
    "\n",
    "**NOTE:** The **DA** object is only used in Databricks Academy courses and is not available outside of these courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654e7309-263a-4456-8314-8090aaf2e402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%run ./Includes/Classroom-Setup-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67b303e8-8cf3-4a0b-a458-b15fd8ddf54e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Create a Starter Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cb236f3-a97c-4ce5-bc27-a24ac568c3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Run the cell below to use the custom `DA` object to create a starter job for this demonstration. After the cell completes, it will create a job named **&lt;your-schema>_Lesson_04&gt;** with three individual tasks.\n",
    "\n",
    "**NOTE:** The following custom method uses the Databricks SDK to programmatically create a job for demonstration purposes. You can find the method definition that uses the Databricks SDK to create the job in the [Classroom-Setup-Common]($./Includes/Classroom-Setup-Common) notebook. However, the [Databricks SDK](https://databricks-sdk-py.readthedocs.io/en/latest/) is outside the scope of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7562906c-674a-4e09-a56f-faa138497719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the job: labuser10356537_1748007971_Lesson_04\nJob ID: 965197437610508\n"
     ]
    }
   ],
   "source": [
    "DA.create_job_lesson04()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8168db44-6448-443f-b11f-ae6009c13fe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Open your new starter job **&lt;your-schema>_Lesson_04&gt;**:\n",
    "  - a. Navigate to **Workflows** and open it in a new tab.\n",
    "  \n",
    "  - b. Select your new job, **&lt;your-schema>_Lesson_04&gt;**.\n",
    "  \n",
    "  - c. Select **Tasks** in the top navigation bar.\n",
    "  \n",
    "  - d. View your job. Notice that the job contains three tasks: **Ingest_Source_1**, **Ingest_Source_2**, and **Ingest_Source_3**. The three tasks should all be independent of one another. \n",
    "\n",
    "  - e. Leave the job open and return to these instructions.\n",
    "\n",
    "  ![Lesson04_DefaultJob](files/images/deploy-workloads-with-databricks-workflows-2.0.2/Lesson04_DefaultJob.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cfb4916-083c-4ab1-931f-e1853f01939e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Explore the Task Notebooks\n",
    "1. The following notebooks use simple code to demonstrate the functionality of creating workflows. Each notebook is in **Task Notebooks** > **Lesson 4 Notebooks**. You can use the links below to view each notebook used in the tasks and explore the code.\n",
    "\n",
    "- Task: [Ingest_Source_1]($./Task Notebooks/Lesson 4 Notebooks/Ingest Source 1) notebook\n",
    "\n",
    "- Task: [Ingest_Source_2]($./Task Notebooks/Lesson 4 Notebooks/Ingest Source 2) notebook\n",
    "\n",
    "- Task: [Ingest_Source_3]($./Task Notebooks/Lesson 4 Notebooks/Ingest Source 3) notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c7e4091-a092-47eb-9339-11f6ea5bbe49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Set Dependencies on the Tasks\n",
    "We are going to make some changes to this job and configure our main task to run only if all previous tasks succeed.\n",
    "\n",
    "To simulate a real-world scenario, we will configure the job to either succeed or fail based on a task parameter. At this point, we want to observe the behavior when an upstream task fails, so we will configure the task parameter to intentionally cause the notebook to fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb8ddf82-65a9-4aca-93ae-b2680aa40ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D1. Add a Parameter to the Ingest_Source_1 Task\n",
    "\n",
    "Configure the task parameter in the **Ingest_Source_1** task as follows:\n",
    "\n",
    "1. Select the task **Ingest_Source_1**.\n",
    "\n",
    "2. In the **Parameters** field, click **Add**.\n",
    "\n",
    "3. For **Key**, type *test_value*.\n",
    "\n",
    "4. For **Value**, type *Failure*.\n",
    "\n",
    "    **NOTE:** Be aware values are case-sensitive\n",
    "\n",
    "5. For **Retries**, select the edit button. In the **Retry Policy**, deselect *Enable serverless auto-optimization (may include at most 3 retries)*. This will turn off the retry policy if the job fails, preventing the task from running multiple times to save time during the demonstration.\n",
    "\n",
    "6. Click **Confirm**.\n",
    "\n",
    "5. Click **Create task**.\n",
    "\n",
    "We now added a parameter to the **Ingest_Source_1** task. \n",
    "\n",
    "**The parameter value  *Failure* will cause an error when this notebook is run.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e8311c9-fb46-40a2-8482-3605091aa302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D2. Add the Clean_Data Task\n",
    "Let's add the **Clean Data** notebook as a task named **Clean_Data**. The notebook will add dynamic value references to the task. \n",
    "\n",
    "Databricks Workflow Jobs provide [options](https://docs.databricks.com/en/workflows/jobs/parameter-value-references.html) for passing information about jobs/tasks to tasks and from one task to another. \n",
    "\n",
    "\n",
    "1. Open and explore the code in the [Clean Data]($./Task Notebooks/Lesson 4 Notebooks/Clean Data) notebook.\n",
    "\n",
    "    **NOTE:** The notebook creates Dynamic value references using `dbutils.jobs.taskValues.set(key = 'bad_records', value = 5)`. This dynamically creates a **Key** named *bad_records* with the **Value** *5*. This can be set dynamically in whatever way you wish.\n",
    "\n",
    "\n",
    "1. Close the **Clean Data** notebook.\n",
    "\n",
    "1. Click **Add task**, and select **Notebook**. \n",
    "\n",
    "1. Name the task **Clean_Data**.\n",
    "\n",
    "1. Click the path field, select the notebook, **Lesson 4 Notebooks/Clean Data**, and click **Confirm**\n",
    "\n",
    "1. Click the **Depends on** field, and select all three of the tasks in the job (**Ingest_Source_1**, **Ingest_Source_2**, **Ingest_Source_3** ) to add them to the list. The DAG should show connections from the first three tasks to the **Clean_Data** task.\n",
    "\n",
    "1. Click **All succeeded** in the **Run if dependencies** field to drop open the combo box. Note the variety of conditions available. \n",
    "\n",
    "1. Select **All succeeded**.\n",
    "\n",
    "1. Click **Save Task**.\n",
    "\n",
    "![Lesson04_Clean_Data_Task](files/images/deploy-workloads-with-databricks-workflows-2.0.2/Lesson04_Clean_Data_Task.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3cbdd88-1664-46e1-b6d8-c0f7159d91ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D3. Run the Job\n",
    "1. Run the job by clicking **Run now** in the upper-right corner. \n",
    "\n",
    "2. A pop-up window appears with a link to the job run. Click **View run**.\n",
    "\n",
    "3. Watch the tasks in the DAG. The colors change to show the progress of the task (about 2-3 minutes to complete):\n",
    "\n",
    "  * **Gray** -- the task has not started\n",
    "  * **Green stripes** -- the task is currently running\n",
    "  * **Solid green** -- the task completed successfully\n",
    "  * **Dark red** -- the task failed\n",
    "  * **Light red** -- an upstream task failed, so the current task never ran\n",
    "\n",
    "4. When the run is finished, note that **Ingest_Source_1** failed. This was expected. Also, note that our **Clean_Data** task never ran because we required that all three parent tasks must succeed before the **Clean_Data** task will run.\n",
    "\n",
    "![Lesson04_Ingest_Source_1_Failure](files/images/deploy-workloads-with-databricks-workflows-2.0.2/Lesson04_Ingest_Source_1_Failure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b92bb947-89f9-42d1-80a2-f3904c3cd557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### D4. Repairing Job Runs\n",
    "\n",
    "We have the ability to view the notebooks used in a task, including their output, as part of the job run. This can help us diagnose errors. We also have the ability to re-run specific tasks in a failed job run. \n",
    "\n",
    "Consider the following example:\n",
    "\n",
    "You are developing a job that includes a handful of notebooks. During a run of the job, one of the tasks fails. You can change the code in that notebook and re-run that task and any tasks that depend on it. Additionally, you can change task parameters and re-run a task. Let's do this now:\n",
    "\n",
    "1. In the upper-right corner, click **Repair run**. \n",
    "\n",
    "2. Make sure the **Ingest_Source_1** task is selected. Change the value for **test_value** from *Failure* to *Succeed*.\n",
    "\n",
    "3. Highlight **Clean_Data** task so it is included in the Repair Run, then click **Repair run (2)** and let the job complete. You'll see Pipeline animate if a few seconds.\n",
    "\n",
    "4. The \"2\" in the **Repair run** button indicates that Databricks selected both the failed task and the task that depended on it. You can select and deselect whichever tasks you wish to re-run.\n",
    "\n",
    "    **NOTE:** When we use **Repair run** to change a parameter, the original parameter in the task definition is not changedâ€”only the parameter in the current run is updated. In next Step, we will hard-code the Task parameter from *Failure* to *Succeed*.\n",
    "\n",
    "5. After the repaired run is executed successfully complete the following:\n",
    "    - a. Navigate back to the job by selecting it in the link above the job name\n",
    "    \n",
    "    - b. Select **Tasks** in the navigation bar.\n",
    "    \n",
    "    - c. Select the **Ingest_Source_1** task and modify the parameter value of **test_value** to **Succeed**\n",
    "    \n",
    "    - d. Then select **Save task**.\n",
    "\n",
    "\n",
    "![Lesson04_RepairRun_01](files/images/deploy-workloads-with-databricks-workflows-2.0.2/Lesson04_RepairRun_01.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4b39bbb-1b45-432f-8cd3-8c530c27aa8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Add If/Else Condition Task\n",
    "We can also perform branching logic based on a boolean condition. Let's see an example of this in action. \n",
    "\n",
    "Suppose our **Clean_Data** task pushes bad records to a quarantine table. We will want to fix these bad records, if possible, before continuing to the next task. Let's set up this logic:\n",
    "\n",
    "1. Go back to the job definition page for our job and make sure you are on the **Tasks** tab.\n",
    "\n",
    "2. Select the **Clean_Data** task and click **Add task**. Then select **If/else condition** (under the **Control flow** section at the bottom of the list).\n",
    "\n",
    "3. Name the task **Is_Record_Check_0**. Note have 2 new settings in node (*True* and *False*).\n",
    "\n",
    "4. For the condition field complete the following:\n",
    "    - a. Type `{{tasks.Clean_Data.values.bad_records}}` (make sure to include both sets of curly brace) in the left-hand side field.  \n",
    "\n",
    "    - b. Choose **`==`** in the dropdown, and type *0* in the right-hand side field. This code obtains the value of the **bad_records** key in the **Clean_Data** task and compares it to the condition.\n",
    "\n",
    "    **NOTE:** To obtain the task value with the key *value_name* that was set by task *task_name* use the following:`{{tasks.task_name.values.value_name}}`. View the [Supported value references](https://docs.databricks.com/en/jobs/dynamic-value-references.html#supported-value-references) for a list of dynamic value references that are supported.\n",
    "\n",
    "5. Ensure **Depends on** is set to **Clean_Data** and **Run if dependencies** is set to **All succeeded**.\n",
    "\n",
    "6. Click **Save task**.\n",
    "\n",
    "![Lesson04_If_Condition](files/images/deploy-workloads-with-databricks-workflows-2.0.2/Lesson04_If_Condition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21442151-c8ae-4f93-9e16-04e305adf280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E1. Setting Our \"False\" Task\n",
    "Let's setup a task that will fix bad records before moving on in the job. That is, if the **bad_records** value is not equal to *0*.:\n",
    "\n",
    "1. Select the **Is_Record_Check_0** task and select **Add task** on the job definition page, then select **Notebook**.\n",
    "\n",
    "1. Name the task **Fix_Bad_Records**.\n",
    "\n",
    "1. For the path, navigate to **Lesson 4 Notebooks/Fix Bad Records**. \n",
    "\n",
    "    **NOTE:** The **Fix_Bad_Records** notebook will simply execute the code: `print(\"Logic that fixes bad records.\")`. In a real world scenario you can set any logic here.\n",
    "\n",
    "1. For **Depends on**, confirm only **Is_Record_Check_0 (false)**. \n",
    "\n",
    "    **NOTE:** Please make sure that **Is_Record_Check_0 (true)** is not selected!\n",
    "\n",
    "1. Ensure that Run if dependencies is set to **All succeeded**.\n",
    "\n",
    "1. Click **Create task**\n",
    "\n",
    "We are setting this task to only run if the **Is_Record_Check_0** fails, meaning the value for the **bad_records** key did not equal 0. In our current scenario, the **Clean_Data** task sets the value of **bad_records** to *5*.\n",
    "\n",
    "\n",
    "![Lesson04_Fix_Bad_Records_False](files/images/deploy-workloads-with-databricks-workflows-2.0.2/Lesson04_Fix_Bad_Records_False.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c56f9473-2a45-4719-8a8e-c9b6b0c5ae15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E2. Aggregate Records Task\n",
    "Let's setup a task for aggregating data. We want this task to either run if:\n",
    "- If the **Is_Record_Check_0** is *true* (meaning `bad_records == 0`) or \n",
    "- If the **Is_Record_Check_0** is *false* (meaning `bad_records != 0`) and the **Fix_Bad_Records** task completed.\n",
    "\n",
    "**NOTE:** The **Aggregate Records** notebook will simply run: `print(\"Aggregates records.\")`. In a real world scenario you can add any logic you need.\n",
    "\n",
    "1. Select the **Fix_Bad_Records** task then select **Add task** and select **Notebook**.\n",
    "\n",
    "1. Name the task **Aggregate_Records**.\n",
    "\n",
    "1. For the path, navigate to **Lesson 4 Notebooks/Aggregate Records**\n",
    "\n",
    "1. For **Depends on**, select **Is_Record_Check_0 (true)** AND **Fix_Bad_Records**.\n",
    "\n",
    "1. Set **Run if dependencies** to **At least one succeeded**.\n",
    "\n",
    "1. Click **Create task**\n",
    "\n",
    "This task will run if either:\n",
    "- **Is_Record_Check_0** is *true*, (which is not the case since 5 does not equal 0) or\n",
    "- **Fix_Bad_Records** completed successfully (this is the case for our example)\n",
    "\n",
    "![Lesson04_FullJob](files/images/deploy-workloads-with-databricks-workflows-2.0.2/Lesson04_FullJob.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df541843-377c-4189-be0f-96487d43499e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E3. Run the Entire Job\n",
    "\n",
    "##### IMPORTANT NOTE\n",
    "Before running the job, go to the **Task** tab on the **Job Details** page and select **Ingest_Source_1**. In the **Parameters** section make sure the key **test_value** is set to *Succeed*.\n",
    "\n",
    "1. Click **Run now** to run the entire job. Then in the pop up select **View Run**.\n",
    "  \n",
    "2. At the moment, our hard-coded **bad_records** value is set to *5*, so we should see the **Fix_Bad_Records** task run before the **Aggregate_Records** task when we run the job.\n",
    "\n",
    "![Lesson04_FullJobRun](files/images/deploy-workloads-with-databricks-workflows-2.0.2/Lesson04_FullJobRun.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8013d752-b034-48cc-a1ed-1f1ea31fb5b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bonus lab: \n",
    "###Change *Is_Record_Check_0* task to fire a *True* condition\n",
    "\n",
    "1. From *Tasks* tab, open the  **Is_Record_Check_0** task.\n",
    "\n",
    "2. In **Condition** caption change 0 to 5.  Now 5 (from **Clean_Data** task) = 5 (in this task). \n",
    "\n",
    "3. For completeness, change the *Task name* to **Is_Record_Check_5**.  This will fire the **True** condition when run and flow through to the **Aggregate_Records** task.  Click on **Run now** button to confirm.\n",
    "\n",
    "\n",
    "##### Final Run\n",
    "![Lesson04_True_run](files/images/deploy-workloads-with-databricks-workflows-2.0.2/Lesson04_True_run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ebb289b-5e73-4334-9057-196cd404cb45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4 - Conditional Tasks and Repair Runs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}